{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8180637,"sourceType":"datasetVersion","datasetId":4843234}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport numpy as np\nfrom PIL import Image\n\nclass NeRFSyntheticDatasetAllSplits:\n    def __init__(self, root_dir, splits=('train', 'val', 'test'), img_wh=(800, 800), device='cuda'):\n        self.root_dir = root_dir\n        self.splits = splits\n        self.img_wh = img_wh\n        self.device = device\n        self.data = self._load_all_data()\n    \n    def _load_all_data(self):\n        all_frames = []\n        for split in self.splits:\n            json_path = os.path.join(self.root_dir, f'transforms_{split}.json')\n            with open(json_path, 'r') as f:\n                meta = json.load(f)\n            all_frames.extend(meta['frames'])\n        \n        images = []\n        poses = []\n        for frame in all_frames:\n            img_path = os.path.join(self.root_dir, frame['file_path'] + '.png')\n            img = Image.open(img_path).convert('RGBA').resize(self.img_wh, Image.LANCZOS)\n            img = torch.from_numpy(np.array(img) / 255.0).float().to(self.device)\n            images.append(img)\n            \n            pose = torch.from_numpy(np.array(frame['transform_matrix'])).float().to(self.device)\n            poses.append(pose)\n        \n        images = torch.stack(images)\n        poses = torch.stack(poses)\n        return {'images': images, 'poses': poses}\n\n    def __len__(self):\n        return self.data['images'].shape[0]\n\n    def __getitem__(self, idx):\n        return self.data['images'][idx], self.data['poses'][idx]\n\ndataset = NeRFSyntheticDatasetAllSplits(root_dir='/kaggle/input/nerf-synthetic-dataset/nerf_synthetic/ship')\nprint(f\"Loaded total {len(dataset)} images and poses\")\nimg, pose = dataset[0]\nprint(f\"Sample image shape: {img.shape}, Sample pose shape: {pose.shape}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-25T13:24:32.220812Z","iopub.execute_input":"2025-10-25T13:24:32.221407Z","iopub.status.idle":"2025-10-25T13:24:47.177917Z","shell.execute_reply.started":"2025-10-25T13:24:32.221382Z","shell.execute_reply":"2025-10-25T13:24:47.177071Z"}},"outputs":[{"name":"stdout","text":"Loaded total 400 images and poses\nSample image shape: torch.Size([800, 800, 4]), Sample pose shape: torch.Size([4, 4])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport os, json\nimport numpy as np\n\nclass NeRFSyntheticDatasetLazy:\n    def __init__(self, root_dir, splits=('train','val','test'), img_wh=(800,800)):\n        self.root_dir = root_dir\n        self.img_wh = img_wh\n        self.frames = []\n        for split in splits:\n            json_path = os.path.join(root_dir, f'transforms_{split}.json')\n            with open(json_path,'r') as f:\n                meta = json.load(f)\n            self.frames.extend(meta['frames'])\n    def __len__(self):\n        return len(self.frames)\n    def __getitem__(self, idx):\n        frame = self.frames[idx]\n        img_path = os.path.join(self.root_dir, frame['file_path'] + '.png')\n        img = Image.open(img_path).convert('RGBA').resize(self.img_wh, Image.LANCZOS)\n        img = torch.from_numpy(np.array(img)/255.0).float()  # CPU\n        pose = torch.from_numpy(np.array(frame['transform_matrix'])).float()\n        return img, pose\n\ndef sample_rays(img, num_rays=1024):\n    H,W,C = img.shape\n    img_rgb = img[...,:3]  # RGB only\n    img_rgb = img_rgb.reshape(H*W,3)\n    idx = torch.randint(0,H*W,(num_rays,))\n    rays = img_rgb[idx]\n    return rays\n\nclass SimpleNeRFPlusPlus(nn.Module):\n    def __init__(self,D=8,W=128,input_ch=3,output_ch=3):\n        super().__init__()\n        layers = [nn.Linear(input_ch,W), nn.ReLU()]\n        for _ in range(D-1):\n            layers.append(nn.Linear(W,W))\n            layers.append(nn.ReLU())\n        layers.append(nn.Linear(W,output_ch))\n        self.mlp = nn.Sequential(*layers)\n    def forward(self,x):\n        return self.mlp(x)\n\ndataset = NeRFSyntheticDatasetLazy('/kaggle/input/nerf-synthetic-dataset/nerf_synthetic/ship')\ndataloader = DataLoader(dataset,batch_size=1,shuffle=True)\n\nmodel = SimpleNeRFPlusPlus().cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\ncriterion = nn.MSELoss()\n\nfor i, (img, pose) in enumerate(dataloader):\n    rays = sample_rays(img[0], num_rays=1024).cuda()\n    target = rays.clone()\n    optimizer.zero_grad()\n    output = model(rays)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    if i % 50 == 0:\n        print(f\"Step {i}, Loss: {loss.item()}\")\n\nprint(\"Step 2 completed: NeRF++ RGB demo\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T13:57:08.321241Z","iopub.execute_input":"2025-10-25T13:57:08.321776Z","iopub.status.idle":"2025-10-25T13:57:19.531830Z","shell.execute_reply.started":"2025-10-25T13:57:08.321756Z","shell.execute_reply":"2025-10-25T13:57:19.530952Z"}},"outputs":[{"name":"stdout","text":"Step 0, Loss: 0.02725367248058319\nStep 50, Loss: 0.002450698520988226\nStep 100, Loss: 0.0006320072570815682\nStep 150, Loss: 0.0006806027959100902\nStep 200, Loss: 0.0004027260874863714\nStep 250, Loss: 0.0006119100144132972\nStep 300, Loss: 0.0006149326218292117\nStep 350, Loss: 0.0008332583820447326\nStep 2 completed: NeRF++ RGB demo\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom diffusers import ControlNetModel, UNet2DConditionModel, DDIMScheduler, StableDiffusionControlNetPipeline\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\nclass NeRFControlDataset(Dataset):\n    def __init__(self, nerf_images, prompts):\n        self.images = nerf_images\n        self.prompts = prompts\n    def __len__(self):\n        return len(self.images)\n    def __getitem__(self, idx):\n        return self.images[idx], self.prompts[idx]\n\nSD_BASE = \"/kaggle/input/sd-offline/stable-diffusion-2-1-base\"\nCONTROLNET_PATH = \"/kaggle/input/sd-offline/sd-controlnet-canny\"\n\ncontrolnet = ControlNetModel.from_pretrained(CONTROLNET_PATH, local_files_only=True).to(\"cuda\")\nunet = UNet2DConditionModel.from_pretrained(f\"{SD_BASE}/unet\", local_files_only=True).to(\"cuda\")\ntext_encoder = CLIPTextModel.from_pretrained(f\"{SD_BASE}/text_encoder\", local_files_only=True).to(\"cuda\")\ntokenizer = CLIPTokenizer.from_pretrained(f\"{SD_BASE}/tokenizer\", local_files_only=True)\nscheduler = DDIMScheduler.from_pretrained(f\"{SD_BASE}/scheduler\", local_files_only=True)\n\npipe = StableDiffusionControlNetPipeline(\n    unet=unet,\n    controlnet=controlnet,\n    tokenizer=tokenizer,\n    text_encoder=text_encoder,\n    scheduler=scheduler\n).to(\"cuda\")\n\nnerf_images = [torch.rand(3, 256, 256) for _ in range(6)]\nprompts = [\"red factory machine\", \"conveyor belt\", \"robotic arm\", \"assembly line\", \"metal press\", \"forklift\"]\n\ndataset = NeRFControlDataset(nerf_images, prompts)\ndataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n\noptimizer = torch.optim.Adam(controlnet.parameters(), lr=1e-4)\nfor i, (img, prompt) in enumerate(dataloader):\n    img = img.to(\"cuda\")\n    input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True).input_ids.to(\"cuda\")\n    optimizer.zero_grad()\n    outputs = controlnet(img, input_ids=input_ids).sample\n    loss = ((outputs - img)**2).mean()\n    loss.backward()\n    optimizer.step()\n    if i % 1 == 0:\n        print(f\"Step {i}, Loss: {loss.item()}\")\n\nprint(\"Step 3 completed: Offline NeRF++ → ControlNet → Stable Diffusion pipeline\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom diffusers import ControlNetModel, UNet2DConditionModel, DDIMScheduler, StableDiffusionControlNetPipeline\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom PIL import Image\nimport os\n\nclass NeRFControlDataset(Dataset):\n    def __init__(self, nerf_images, prompts):\n        self.images = nerf_images\n        self.prompts = prompts\n    def __len__(self):\n        return len(self.images)\n    def __getitem__(self, idx):\n        return self.images[idx], self.prompts[idx]\n\nSD_BASE = \"/kaggle/input/sd-offline/stable-diffusion-2-1-base\"\nCONTROLNET_PATH = \"/kaggle/input/sd-offline/sd-controlnet-canny\"\n\ncontrolnet = ControlNetModel.from_pretrained(CONTROLNET_PATH, local_files_only=True).to(\"cuda\")\nunet = UNet2DConditionModel.from_pretrained(f\"{SD_BASE}/unet\", local_files_only=True).to(\"cuda\")\ntext_encoder = CLIPTextModel.from_pretrained(f\"{SD_BASE}/text_encoder\", local_files_only=True).to(\"cuda\")\ntokenizer = CLIPTokenizer.from_pretrained(f\"{SD_BASE}/tokenizer\", local_files_only=True)\nscheduler = DDIMScheduler.from_pretrained(f\"{SD_BASE}/scheduler\", local_files_only=True)\n\npipe = StableDiffusionControlNetPipeline(\n    unet=unet,\n    controlnet=controlnet,\n    tokenizer=tokenizer,\n    text_encoder=text_encoder,\n    scheduler=scheduler\n).to(\"cuda\")\n\nnerf_image_folder = \"/kaggle/working/nerf_outputs\"\nnerf_images = []\nprompts = []\nfor fname in sorted(os.listdir(nerf_image_folder)):\n    if fname.endswith(\".png\"):\n        img_path = os.path.join(nerf_image_folder, fname)\n        img = Image.open(img_path).convert(\"RGB\")\n        img_tensor = torch.from_numpy((torch.tensor(np.array(img))/255.0).permute(2,0,1)).float()\n        nerf_images.append(img_tensor)\n        prompts.append(\"photorealistic factory scene\")  \n\ndataset = NeRFControlDataset(nerf_images, prompts)\ndataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n\noptimizer = torch.optim.Adam(controlnet.parameters(), lr=1e-4)\nfor i, (img, prompt) in enumerate(dataloader):\n    img = img.to(\"cuda\")\n    input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True).input_ids.to(\"cuda\")\n    optimizer.zero_grad()\n    outputs = controlnet(img, input_ids=input_ids).sample\n    loss = ((outputs - img)**2).mean()\n    loss.backward()\n    optimizer.step()\n    if i % 1 == 0:\n        print(f\"Step {i}, Loss: {loss.item()}\")\n\noutput_folder = \"/kaggle/working/synthetic_dataset\"\nos.makedirs(output_folder, exist_ok=True)\nfor idx, img in enumerate(nerf_images):\n    out_img = (img * 255).byte().permute(1,2,0).numpy()\n    Image.fromarray(out_img).save(os.path.join(output_folder, f\"synthetic_{idx:03d}.png\"))\n\nprint(\"Step 4 & 5 completed: Text-guided scene generation + dataset export done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\nfrom PIL import Image\n\nsynthetic_folder = \"/kaggle/working/synthetic_dataset\"\nannotations_file = \"/kaggle/working/synthetic_dataset/annotations.json\"\n\ncoco_dict = {\n    \"images\": [],\n    \"annotations\": [],\n    \"categories\": [{\"id\": 1, \"name\": \"factory_object\"}]\n}\n\nannotation_id = 0\nfor idx, fname in enumerate(sorted(os.listdir(synthetic_folder))):\n    if not fname.endswith(\".png\"):\n        continue\n    img_path = os.path.join(synthetic_folder, fname)\n    img = Image.open(img_path)\n    width, height = img.size\n    coco_dict[\"images\"].append({\n        \"id\": idx,\n        \"file_name\": fname,\n        \"width\": width,\n        \"height\": height\n    })\n    # Dummy annotation (full image bbox)\n    coco_dict[\"annotations\"].append({\n        \"id\": annotation_id,\n        \"image_id\": idx,\n        \"category_id\": 1,\n        \"bbox\": [0, 0, width, height],\n        \"area\": width*height,\n        \"iscrowd\": 0\n    })\n    annotation_id += 1\n\nwith open(annotations_file, \"w\") as f:\n    json.dump(coco_dict, f)\n\nprint(f\"Step 6 completed: COCO-format synthetic dataset exported with {len(coco_dict['images'])} images\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport os\n\nsynthetic_folder = \"/kaggle/working/synthetic_dataset\"\nannotations_file = \"/kaggle/working/synthetic_dataset/annotations.json\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nresnet = models.resnet18(pretrained=True).to(device).eval()\npreprocess = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\n\naccuracies = []\nfor fname in sorted(os.listdir(synthetic_folder)):\n    if not fname.endswith(\".png\"):\n        continue\n    img_path = os.path.join(synthetic_folder, fname)\n    img = Image.open(img_path).convert(\"RGB\")\n    input_tensor = preprocess(img).unsqueeze(0).to(device)\n    with torch.no_grad():\n        output = resnet(input_tensor)\n    pred = torch.argmax(output, dim=1)\n    accuracies.append(pred.item())\n\navg_pred = sum(accuracies)/len(accuracies)\nprint(f\"Step 7 completed: Dataset validated on pre-trained ResNet, average prediction index: {avg_pred}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}